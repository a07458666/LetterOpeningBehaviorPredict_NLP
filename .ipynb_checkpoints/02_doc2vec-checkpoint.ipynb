{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/project/at082-group17'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from gensim.models import Doc2Vec, doc2vec\n",
    "from gensim.models import word2vec\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "## turn back to main directory\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load 'article_cutted'\n",
    "with open('data/mail_cutted', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a document id map\n",
    "sentence_list = []\n",
    "\n",
    "for i, l in enumerate(data):\n",
    "    sentence_list.append(doc2vec.LabeledSentence(words=l, tags=[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabeledSentence(['禾宸', '生醫', '國際', '有限公司', '台灣', '原生', '山', '芙蓉', '修護', '保濕霜', '立即', '體驗', '用品', '申請'], ['2'])\n",
      "len 30627\n"
     ]
    }
   ],
   "source": [
    "print(sentence_list[2])\n",
    "print('len ' + str(len(sentence_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define 轉換器\n",
    "model = Doc2Vec(size=256, min_count=5, window=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 14:36:16,512: INFO: collecting all words and their counts\n",
      "2019-10-25 14:36:16,513: INFO: PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-10-25 14:36:16,572: INFO: PROGRESS: at example #10000, processed 123541 words (2124970/s), 17560 word types, 10000 tags\n",
      "2019-10-25 14:36:16,629: INFO: PROGRESS: at example #20000, processed 244943 words (2125983/s), 27240 word types, 20000 tags\n",
      "2019-10-25 14:36:16,683: INFO: PROGRESS: at example #30000, processed 367665 words (2290970/s), 34610 word types, 30000 tags\n",
      "2019-10-25 14:36:16,687: INFO: collected 35068 word types and 30627 unique tags from a corpus of 30627 examples and 375546 words\n",
      "2019-10-25 14:36:16,687: INFO: Loading a fresh vocabulary\n",
      "2019-10-25 14:36:16,712: INFO: min_count=5 retains 9331 unique words (26% of original 35068, drops 25737)\n",
      "2019-10-25 14:36:16,713: INFO: min_count=5 leaves 333053 word corpus (88% of original 375546, drops 42493)\n",
      "2019-10-25 14:36:16,736: INFO: deleting the raw counts dictionary of 35068 items\n",
      "2019-10-25 14:36:16,737: INFO: sample=0.001 downsamples 24 most-common words\n",
      "2019-10-25 14:36:16,739: INFO: downsampling leaves estimated 323032 word corpus (97.0% of prior 333053)\n",
      "2019-10-25 14:36:16,740: INFO: estimated required memory for 9331 words and 256 dimensions: 61262836 bytes\n",
      "2019-10-25 14:36:16,759: INFO: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "## build vocabulary\n",
    "model.build_vocab(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 14:36:19,830: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:20,876: INFO: PROGRESS: at 82.60% examples, 280866 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:21,012: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:21,042: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:21,046: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:21,046: INFO: training on 375546 raw words (353629 effective words) took 1.2s, 292475 effective words/s\n",
      "2019-10-25 14:36:21,071: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:22,086: INFO: PROGRESS: at 79.70% examples, 279900 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:22,276: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:22,281: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:22,295: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:22,295: INFO: training on 375546 raw words (353477 effective words) took 1.2s, 290420 effective words/s\n",
      "2019-10-25 14:36:22,320: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:23,336: INFO: PROGRESS: at 80.08% examples, 280837 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:23,516: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:23,531: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:23,535: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:23,536: INFO: training on 375546 raw words (353695 effective words) took 1.2s, 293422 effective words/s\n",
      "2019-10-25 14:36:23,560: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:24,595: INFO: PROGRESS: at 82.51% examples, 284745 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:24,754: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:24,761: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:24,774: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:24,774: INFO: training on 375546 raw words (353732 effective words) took 1.2s, 293826 effective words/s\n",
      "2019-10-25 14:36:24,802: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:25,838: INFO: PROGRESS: at 82.54% examples, 284219 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:26,004: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:26,010: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:26,026: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:26,027: INFO: training on 375546 raw words (353624 effective words) took 1.2s, 290742 effective words/s\n",
      "2019-10-25 14:36:26,051: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:27,062: INFO: PROGRESS: at 79.89% examples, 281423 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:27,236: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:27,256: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:27,260: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:27,260: INFO: training on 375546 raw words (353589 effective words) took 1.2s, 294244 effective words/s\n",
      "2019-10-25 14:36:27,285: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:28,304: INFO: PROGRESS: at 80.00% examples, 279005 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:28,477: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:28,498: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:28,502: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:28,503: INFO: training on 375546 raw words (353730 effective words) took 1.2s, 292278 effective words/s\n",
      "2019-10-25 14:36:28,527: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:29,574: INFO: PROGRESS: at 82.44% examples, 280631 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:29,699: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:29,732: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:29,741: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:29,742: INFO: training on 375546 raw words (353690 effective words) took 1.2s, 292927 effective words/s\n",
      "2019-10-25 14:36:29,766: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:30,816: INFO: PROGRESS: at 82.55% examples, 280579 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:30,951: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:30,973: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:30,984: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:30,985: INFO: training on 375546 raw words (353611 effective words) took 1.2s, 292487 effective words/s\n",
      "2019-10-25 14:36:31,012: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:32,076: INFO: PROGRESS: at 82.39% examples, 275704 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:32,190: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:32,216: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:32,229: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:32,229: INFO: training on 375546 raw words (353632 effective words) took 1.2s, 292089 effective words/s\n",
      "2019-10-25 14:36:32,256: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:33,326: INFO: PROGRESS: at 82.50% examples, 274463 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:33,454: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:33,468: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:33,485: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:33,485: INFO: training on 375546 raw words (353740 effective words) took 1.2s, 289401 effective words/s\n",
      "2019-10-25 14:36:33,510: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:34,520: INFO: PROGRESS: at 82.58% examples, 291026 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:34,700: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:34,707: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:34,723: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:34,724: INFO: training on 375546 raw words (353638 effective words) took 1.2s, 293157 effective words/s\n",
      "2019-10-25 14:36:34,749: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:35,821: INFO: PROGRESS: at 82.44% examples, 274717 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:35,931: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:35,959: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:35,967: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:35,968: INFO: training on 375546 raw words (353632 effective words) took 1.2s, 292565 effective words/s\n",
      "2019-10-25 14:36:35,993: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:37,046: INFO: PROGRESS: at 82.52% examples, 279627 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:37,161: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:37,192: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:37,197: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:37,198: INFO: training on 375546 raw words (353618 effective words) took 1.2s, 296094 effective words/s\n",
      "2019-10-25 14:36:37,223: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:38,284: INFO: PROGRESS: at 82.50% examples, 277512 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:38,398: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:38,423: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:38,435: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:38,435: INFO: training on 375546 raw words (353683 effective words) took 1.2s, 294408 effective words/s\n",
      "2019-10-25 14:36:38,460: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:39,507: INFO: PROGRESS: at 82.39% examples, 280270 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:39,631: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:39,655: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:39,666: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:39,666: INFO: training on 375546 raw words (353590 effective words) took 1.2s, 294591 effective words/s\n",
      "2019-10-25 14:36:39,691: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:40,751: INFO: PROGRESS: at 82.47% examples, 277323 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:40,857: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:40,893: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:40,907: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:40,908: INFO: training on 375546 raw words (353634 effective words) took 1.2s, 292298 effective words/s\n",
      "2019-10-25 14:36:40,934: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:41,963: INFO: PROGRESS: at 82.60% examples, 285731 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:42,119: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:42,132: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:42,140: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:42,140: INFO: training on 375546 raw words (353671 effective words) took 1.2s, 294989 effective words/s\n",
      "2019-10-25 14:36:42,165: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:43,181: INFO: PROGRESS: at 79.69% examples, 280738 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-25 14:36:43,379: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:43,383: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:43,388: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:43,388: INFO: training on 375546 raw words (353717 effective words) took 1.2s, 291675 effective words/s\n",
      "2019-10-25 14:36:43,414: INFO: training model with 3 workers on 9331 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-10-25 14:36:44,457: INFO: PROGRESS: at 82.48% examples, 281525 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-25 14:36:44,615: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:36:44,627: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:36:44,633: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:36:44,633: INFO: training on 375546 raw words (353618 effective words) took 1.2s, 291706 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model ; shuffle data every epoch\n",
    "for i in range(20):\n",
    "    random.shuffle(sentence_list)\n",
    "    model.train(sentence_list, total_examples=len(data), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01375487, -0.03106267, -0.0139956 , -0.00157345, -0.03141049,\n",
       "       -0.00695845, -0.00403368,  0.05626339,  0.0271602 , -0.025943  ,\n",
       "       -0.03308766, -0.00260348, -0.00198258,  0.04582683,  0.01950553,\n",
       "        0.0323155 ,  0.01126987, -0.01220672, -0.01324467, -0.01124122,\n",
       "        0.05767435,  0.03684979,  0.01266033, -0.00454513, -0.07179745,\n",
       "       -0.02929165, -0.01229419, -0.00072474, -0.03365819, -0.01494888,\n",
       "        0.03998829, -0.03554944,  0.05063399, -0.00902871,  0.00506555,\n",
       "        0.0416887 ,  0.01878985, -0.00254521, -0.01024618, -0.00815893,\n",
       "        0.03655525,  0.01410494, -0.00423394, -0.02918228,  0.00759457,\n",
       "       -0.01727829,  0.02157821,  0.0092843 ,  0.0547848 ,  0.00065562,\n",
       "        0.04288874,  0.01862917,  0.02817238,  0.02115024,  0.01871689,\n",
       "        0.03356244,  0.01451109, -0.01063997,  0.00969537,  0.02393718,\n",
       "       -0.03668919,  0.00466407, -0.03344009,  0.04297008,  0.02457275,\n",
       "        0.00185318,  0.00543468,  0.01950383,  0.05449451,  0.015801  ,\n",
       "       -0.01199273,  0.01785715,  0.00678035, -0.01869586,  0.05071079,\n",
       "        0.02981838, -0.0063858 , -0.02741321, -0.02435268, -0.00187161,\n",
       "        0.03053985, -0.00832895,  0.00040497,  0.00087759, -0.03579178,\n",
       "       -0.04212123, -0.00625309,  0.0038754 , -0.00701723,  0.06662045,\n",
       "       -0.02724783,  0.00241139, -0.01171205, -0.01547625, -0.00063367,\n",
       "       -0.000739  , -0.04420239, -0.01695987, -0.01531364,  0.05812794,\n",
       "        0.00975197,  0.01496157,  0.01124458, -0.01281431, -0.00358322,\n",
       "       -0.00519017,  0.00662519, -0.01680541, -0.02276032,  0.01095909,\n",
       "        0.02373005,  0.0208549 ,  0.03246345,  0.02356822,  0.00071339,\n",
       "        0.04676436, -0.02337928, -0.05104082,  0.01383805,  0.01553977,\n",
       "        0.07648306,  0.02057553, -0.00460818,  0.03970448,  0.00925208,\n",
       "       -0.03722284, -0.00896494,  0.04610911,  0.03716569,  0.01595275,\n",
       "       -0.05715511, -0.02348661, -0.00661131,  0.02490486, -0.02672755,\n",
       "        0.0471156 , -0.03652135, -0.01794364, -0.01781411,  0.01236542,\n",
       "        0.00995553, -0.00433685,  0.00482085, -0.05610916,  0.01098589,\n",
       "        0.0713554 , -0.01561458, -0.0034581 ,  0.00741101, -0.03640863,\n",
       "        0.00663419,  0.03122865, -0.04414907,  0.01836446, -0.0299743 ,\n",
       "       -0.0093473 , -0.02186151,  0.01321387,  0.01725554,  0.04922928,\n",
       "        0.02847131, -0.02652539,  0.03265903,  0.01018013,  0.01236312,\n",
       "        0.01148147,  0.01025801, -0.08304607, -0.08098464, -0.03098581,\n",
       "        0.0176619 , -0.00161152,  0.02983143,  0.01746982,  0.02674127,\n",
       "        0.03185859,  0.02492257, -0.01389989, -0.00991878,  0.025214  ,\n",
       "       -0.01930393,  0.06329167,  0.05960198,  0.01646046, -0.00875472,\n",
       "       -0.04265163, -0.02105153,  0.01638559, -0.00810709, -0.02365351,\n",
       "        0.00725142,  0.00571207, -0.00812073, -0.00424265, -0.03100495,\n",
       "       -0.04039005, -0.04001714, -0.01400823,  0.01436007, -0.02709571,\n",
       "        0.01952418, -0.02469761, -0.00148698, -0.05333661, -0.01699481,\n",
       "       -0.00242036,  0.01660811,  0.00345071,  0.02928164,  0.0157434 ,\n",
       "       -0.01403909, -0.0443307 ,  0.00249792, -0.01745672, -0.01376711,\n",
       "       -0.0084885 , -0.05390684, -0.00357046,  0.01152005, -0.03851597,\n",
       "       -0.02793017, -0.01166452, -0.03757911,  0.00561839,  0.03134504,\n",
       "       -0.01210359, -0.02452062,  0.01124117,  0.0110339 ,  0.00363634,\n",
       "       -0.05850592,  0.00557057,  0.00114586, -0.04970894, -0.01585271,\n",
       "       -0.03719351, -0.01042338,  0.0100664 , -0.02824496, -0.00101749,\n",
       "       -0.01150079, -0.04368615, -0.00706235,  0.02460741, -0.02017521,\n",
       "       -0.01062037,  0.01107048,  0.0280502 ,  0.00554547,  0.01796377,\n",
       "        0.00846193, -0.00642854, -0.01100871,  0.00219864,  0.00868507,\n",
       "       -0.00911395], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print result\n",
    "model.docvecs['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-23 16:33:11,165: INFO: saving Doc2Vec object under AT082_17_Leopard_Read/model/doc2vec, separately None\n",
      "2019-10-23 16:33:11,166: INFO: not storing attribute syn0norm\n",
      "2019-10-23 16:33:11,167: INFO: not storing attribute cum_table\n",
      "2019-10-23 16:33:13,505: INFO: saved AT082_17_Leopard_Read/model/doc2vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "## save result\n",
    "model.save('AT082_17_Leopard_Read/model/doc2vec')\n",
    "print(\"model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
